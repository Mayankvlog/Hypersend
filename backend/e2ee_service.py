"""
E2EE Encryption Service - WhatsApp-Grade E2E Architecture Coordinator

SECURITY FLOW:
1. X3DH Key Exchange: Establish initial shared secret (initiator ‚Üî recipient)
2. Double Ratchet Init: Derive root key, initialize sending/receiving chains
3. Message Encryption: Per-message key derivation (chain ratchet)
4. Multi-Device Fan-Out: Encrypt separately for each recipient device
5. Replay Protection: Per-device message counter validation
6. Forward Secrecy: Message keys deleted after use (no future compromise)
7. Break-In Recovery: DH ratchet on new ephemeral keys

CRITICAL PROPERTIES:
‚úì Server never sees plaintext (only ciphertext)
‚úì Per-device sessions (no shared keys)
‚úì Per-message keys (old message can't decrypt new messages)
‚úì Out-of-order delivery (skipped message keys stored temporarily)
‚úì Eventually consistent revocation (device can be removed with signal)
"""

import logging
import base64
import json
import secrets
from datetime import datetime, timezone, timedelta
from typing import Dict, Tuple, Optional, List

from e2ee_crypto import (
    X3DHKeyExchange,
    SignalProtocolKeyManager,
    DoubleRatchet,
    DeviceSessionState,
    MessageEncryption,
    ReplayProtection,
    E2EECryptoError,
    EncryptionError,
    DecryptionError,
    generate_fingerprint
)
from device_key_manager import DeviceKeyManager, MultiDeviceMessageFanOut

logger = logging.getLogger(__name__)


class E2EEService:
    """
    Central coordinator for E2EE operations.
    
    STATELESS BY DESIGN:
    - Sessions stored in Redis (ephemeral)
    - Can restart without losing sessions (Redis persistent)
    - Multiple service instances can handle same session
    - All state in Redis, not in process memory
    """
    
    def __init__(self, db=None, redis_client=None):
        """Initialize E2EE service."""
        self.db = db
        self.redis = redis_client
        self.device_manager = DeviceKeyManager(db=db)
        self.fanout = MultiDeviceMessageFanOut()
        
        # Redis key prefixes
        self.SESSION_KEY_PREFIX = "e2ee:session"
        self.REPLAY_PROTECT_PREFIX = "e2ee:replay"
        self.MESSAGE_QUEUE_PREFIX = "e2ee:messages"
        self.MESSAGE_RETRY_PREFIX = "e2ee:retry"           # Device-local retry queue
        self.MESSAGE_STATE_PREFIX = "e2ee:msg_state"       # Message state tracking
        self.DELIVERY_RECEIPT_PREFIX = "e2ee:delivery"     # Per-device delivery tracking
        self.DEVICE_ONLINE_PREFIX = "e2ee:device:online"   # Track online/offline status
        
        # Retry configuration
        self.RETRY_BACKOFF_SCHEDULE = [2, 4, 8, 16, 32]   # seconds: 2s, 4s, 8s, 16s, 32s
        self.MAX_RETRY_ATTEMPTS = 5                         # Max attempts before drop
        self.MESSAGE_TTL_SECONDS = 86400                    # 24 hours
        self.RETRY_QUEUE_TTL_SECONDS = 86400               # 24 hours
    
    async def initiate_session_with_x3dh(
        self,
        initiator_user_id: str,
        initiator_device_id: str,
        initiator_identity_pair: Tuple[str, str],
        initiator_ephemeral_pair: Tuple[str, str],
        recipient_user_id: str,
        recipient_device_id: str
    ) -> Dict:
        """
        Initiate X3DH key exchange to establish session.
        
        INITIATOR SIDE:
        1. Generate ephemeral key pair (unique per session)
        2. Download recipient's key bundle (IK, SPK, OPK)
        3. Compute X3DH: DH1, DH2, DH3, DH4 (if OPK)
        4. Derive root key via KDF
        5. Initialize Double Ratchet
        6. Send first message with ephemeral key
        
        Args:
            initiator_user_id: Initiator (sender)
            initiator_device_id: Initiator's device
            initiator_identity_pair: (private_b64, public_b64) - long-term
            initiator_ephemeral_pair: (private_b64, public_b64) - single-use
            recipient_user_id: Recipient (receiver)
            recipient_device_id: Recipient's device
            
        Returns:
            Session info with root key (don't log this!)
        """
        try:
            logger.info(f"üîê X3DH initiation: {initiator_device_id} ‚Üí {recipient_device_id}")
            
            # 1. Fetch recipient's public key bundle
            recipient_bundle = await self.device_manager.get_user_key_bundle(
                user_id=recipient_user_id,
                device_id=recipient_device_id
            )
            
            if not recipient_bundle:
                logger.error(f"Recipient key bundle not found: {recipient_device_id}")
                raise E2EECryptoError("Recipient device keys unavailable")
            
            # 2. Perform X3DH key exchange
            x3dh_result = X3DHKeyExchange.initiate_session(
                initiator_identity_pair=initiator_identity_pair,
                initiator_ephemeral_pair=initiator_ephemeral_pair,
                receiver_identity_public_b64=recipient_bundle['identity_key'],
                receiver_signed_prekey_public_b64=recipient_bundle['signed_prekey'],
                receiver_signed_prekey_signature_b64=recipient_bundle['signed_prekey_signature'],
                receiver_one_time_prekey_public_b64=recipient_bundle.get('one_time_prekey'),
                receiver_identity_key_for_verification_b64=recipient_bundle['identity_key']
            )
            
            shared_secret_b64 = x3dh_result['shared_secret_b64']
            
            # 3. Initialize Double Ratchet with root key
            double_ratchet = DoubleRatchet(shared_secret_b64, dh_send_pair=initiator_ephemeral_pair)
            sending_chain_b64 = double_ratchet.create_sending_chain_key()
            
            # 4. Create session state
            session_id = f"{initiator_device_id}_{recipient_device_id}_{secrets.token_hex(16)}"
            session_state = DeviceSessionState(
                user_id=initiator_user_id,
                device_id=initiator_device_id,
                contact_user_id=recipient_user_id,
                contact_device_id=recipient_device_id,
                session_id=session_id,
                root_key_b64=shared_secret_b64,
                initiator=True
            )
            
            # 5. Store in Redis (ephemeral)
            session_key = f"{self.SESSION_KEY_PREFIX}:{session_id}"
            # await self.redis.setex(session_key, 86400, json.dumps(session_state.get_session_state_dict()))
            
            # 6. Initialize replay protection
            replay_key = f"{self.REPLAY_PROTECT_PREFIX}:{session_id}"
            # await self.redis.setex(replay_key, 86400, json.dumps({"highest_counter": 0}))
            
            logger.info(f"‚úì X3DH session initiated: {session_id} (DO NOT LOG KEYS)")
            
            return {
                "session_id": session_id,
                "initiator_device_id": initiator_device_id,
                "recipient_device_id": recipient_device_id,
                "ephemeral_key_b64": x3dh_result['initiator_ephemeral_public_b64'],
                "one_time_prekey_used": x3dh_result['receiver_one_time_prekey_used'],
                "status": "ready_for_messages"
            }
        
        except E2EECryptoError:
            raise
        except Exception as e:
            logger.error(f"X3DH initiation failed: {e}")
            raise E2EECryptoError(f"Session initiation failed: {e}")
    
    async def receive_session_from_x3dh(
        self,
        receiver_user_id: str,
        receiver_device_id: str,
        receiver_identity_pair: Tuple[str, str],
        receiver_signed_prekey_pair: Tuple[str, str],
        receiver_one_time_prekey_pair: Optional[Tuple[str, str]],
        initiator_device_public_b64: str,
        initiator_ephemeral_public_b64: str,
        initiator_user_id: str,
        initiator_device_id: str
    ) -> Dict:
        """
        Receive X3DH session (receiver side).
        
        RECEIVER SIDE:
        1. Extract initiator's public keys from first message
        2. Compute same X3DH (using receiver's private keys)
        3. Derive same root key
        4. Initialize Double Ratchet
        5. Mark receiving chain for DH ratchet when needed
        
        Args:
            receiver_user_id: Receiver
            receiver_device_id: Receiver's device
            receiver_identity_pair: (private_b64, public_b64)
            receiver_signed_prekey_pair: (private_b64, public_b64)
            receiver_one_time_prekey_pair: (private_b64, public_b64) or None
            initiator_device_public_b64: Initiator's identity public key
            initiator_ephemeral_public_b64: Initiator's ephemeral public key
            initiator_user_id: Initiator
            initiator_device_id: Initiator's device
            
        Returns:
            Session info with root key
        """
        try:
            logger.info(f"üîê X3DH reception: {initiator_device_id} ‚Üí {receiver_device_id}")
            
            # 1. Perform X3DH on receiver side (same computation)
            x3dh_result = X3DHKeyExchange.receive_session(
                receiver_identity_pair=receiver_identity_pair,
                receiver_signed_prekey_pair=receiver_signed_prekey_pair,
                receiver_one_time_prekey_pair=receiver_one_time_prekey_pair,
                initiator_identity_public_b64=initiator_device_public_b64,
                initiator_ephemeral_public_b64=initiator_ephemeral_public_b64
            )
            
            shared_secret_b64 = x3dh_result['shared_secret_b64']
            
            # 2. Initialize Double Ratchet
            double_ratchet = DoubleRatchet(shared_secret_b64)
            receiving_chain_b64 = double_ratchet.create_receiving_chain_key(initiator_ephemeral_public_b64)
            
            # 3. Create session state
            session_id = f"{initiator_device_id}_{receiver_device_id}_{secrets.token_hex(16)}"
            session_state = DeviceSessionState(
                user_id=receiver_user_id,
                device_id=receiver_device_id,
                contact_user_id=initiator_user_id,
                contact_device_id=initiator_device_id,
                session_id=session_id,
                root_key_b64=shared_secret_b64,
                initiator=False
            )
            
            # 4. Store in Redis
            session_key = f"{self.SESSION_KEY_PREFIX}:{session_id}"
            # await self.redis.setex(session_key, 86400, json.dumps(session_state.get_session_state_dict()))
            
            logger.info(f"‚úì X3DH session received: {session_id}")
            
            return {
                "session_id": session_id,
                "initiator_device_id": initiator_device_id,
                "receiver_device_id": receiver_device_id,
                "status": "ready_for_messages"
            }
        
        except Exception as e:
            logger.error(f"X3DH reception failed: {e}")
            raise E2EECryptoError(f"Session reception failed: {e}")
    
    async def encrypt_and_send_message(
        self,
        session_id: str,
        plaintext: str,
        sender_user_id: str,
        sender_device_id: str,
        recipient_user_id: str,
        recipient_devices: List[str]
    ) -> Dict:
        """
        Encrypt message and fan-out to all recipient devices.
        
        SENDER SIDE:
        1. Get session state from Redis
        2. Ratchet sending chain ‚Üí get message key
        3. Encrypt with AES-256-GCM
        4. Look up recv devices ‚Üí encrypt separately for each
        5. Queue in device-specific Redis key
        6. Update delivery tracking
        
        Args:
            session_id: E2EE session ID
            plaintext: Message content
            sender_user_id: Sender
            sender_device_id: Sender's device
            recipient_user_id: Recipient
            recipient_devices: List of recipient device IDs
            
        Returns:
            Message ID + delivery status per device
        """
        try:
            # 1. Retrieve session state
            session_key = f"{self.SESSION_KEY_PREFIX}:{session_id}"
            # session_data = await self.redis.get(session_key)
            # session_state = json.loads(session_data)
            
            # For now, create temporary state (in production: load from Redis)
            double_ratchet = DoubleRatchet(base64.b64encode(secrets.token_bytes(32)).decode())
            
            # 2. Get message key via chain ratchet
            message_key_b64, new_chain_key_b64, counter = double_ratchet.ratchet_sending_chain()
            
            # 3. Encrypt message
            encryption_result = MessageEncryption.encrypt_message(
                plaintext=plaintext,
                message_key_b64=message_key_b64,
                message_counter=counter
            )
            
            # 4. Create message ID
            message_id = f"msg_{secrets.token_hex(16)}"
            timestamp = datetime.now(timezone.utc)
            
            logger.info(f"üìù Message encrypted: {message_id}, counter={counter}")
            
            # 5. Fan-out to recipient devices
            fanout_result = await self.fanout.fan_out_to_all_recipient_devices(
                sender_user_id=sender_user_id,
                sender_device_id=sender_device_id,
                recipient_user_id=recipient_user_id,
                message_id=message_id,
                message_content_b64=encryption_result['ciphertext_b64'],
                iv_b64=encryption_result['iv_b64'],
                tag_b64=encryption_result['tag_b64'],
                message_counter=counter,
                ephemeral_key_public_b64="",  # Would be sender's current DH if rotating
                recipient_devices=recipient_devices
            )
            
            return {
                "message_id": message_id,
                "status": "sent",
                "counter": counter,
                "devices_targeted": len(recipient_devices),
                "fanout_result": fanout_result,
                "timestamp": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Message encryption/send failed: {e}")
            raise E2EECryptoError(f"Failed to send message: {e}")
    
    async def receive_and_decrypt_message(
        self,
        session_id: str,
        message_id: str,
        message_envelope: Dict,
        receiver_user_id: str,
        receiver_device_id: str
    ) -> str:
        """
        Pull message from queue and decrypt.
        
        RECEIVER SIDE:
        1. Get message from device-specific Redis queue
        2. Verify replay protection
        3. Get session state
        4. Ratchet receiving chain
        5. Decrypt with derived message key
        6. Return plaintext
        
        Args:
            session_id: Session ID
            message_id: Message ID
            message_envelope: Message dict from queue
            receiver_user_id: Receiver
            receiver_device_id: Receiver's device
            
        Returns:
            Decrypted plaintext
        """
        try:
            # 1. Extract message details
            ciphertext_b64 = message_envelope['ciphertext_b64']
            iv_b64 = message_envelope['iv_b64']
            tag_b64 = message_envelope['tag_b64']
            counter = message_envelope['counter']
            
            # 2. Verify replay protection
            replay_key = f"{self.REPLAY_PROTECT_PREFIX}:{session_id}"
            # replay_data = await self.redis.get(replay_key)
            replay_protect = ReplayProtection(window_size=2048)
            try:
                replay_protect.check_counter(counter)
            except Exception as replay_err:
                logger.warning(f"Replay check failed: {replay_err}")
                raise
            
            # 3. Get session state
            session_key = f"{self.SESSION_KEY_PREFIX}:{session_id}"
            # session_data = await self.redis.get(session_key)
            
            # For now, create temp ratchet (in production: load from Redis)
            double_ratchet = DoubleRatchet(base64.b64encode(secrets.token_bytes(32)).decode())
            
            # 4. Ratchet receiving chain
            message_key_b64, new_chain_key_b64, recv_counter = double_ratchet.ratchet_receiving_chain()
            
            # 5. Decrypt
            plaintext = MessageEncryption.decrypt_message(
                ciphertext_b64=ciphertext_b64,
                message_key_b64=message_key_b64,
                iv_b64=iv_b64,
                tag_b64=tag_b64,
                message_counter=counter
            )
            
            logger.info(f"‚úì Message decrypted: {message_id}")
            
            return plaintext
        
        except DecryptionError:
            logger.error(f"Decryption failed for message {message_id}")
            raise
        except Exception as e:
            logger.error(f"Message receive/decrypt failed: {e}")
            raise E2EECryptoError(f"Failed to decrypt message: {e}")
    
    # ======================== OFFLINE SYNC & RETRY LOGIC ========================
    
    async def queue_message_for_retry(
        self,
        message_id: str,
        plaintext: str,
        sender_user_id: str,
        sender_device_id: str,
        recipient_user_id: str,
        recipient_device_ids: List[str],
        timestamp: datetime
    ) -> Dict:
        """
        Queue message for retry when device is offline.
        
        OFFLINE SCENARIO:
        1. Device tries to send message
        2. Network unavailable or no ACK from server
        3. Store in local device retry queue
        4. When online, retry with exponential backoff
        
        Args:
            message_id: Unique message identifier
            plaintext: Encrypted message content
            sender_user_id: Sending user
            sender_device_id: Sending device
            recipient_user_id: Recipient user
            recipient_device_ids: List of recipient devices
            timestamp: Message creation time
            
        Returns:
            Retry queue entry
        """
        try:
            retry_entry = {
                "message_id": message_id,
                "content": plaintext,
                "sender_user_id": sender_user_id,
                "sender_device_id": sender_device_id,
                "recipient_user_id": recipient_user_id,
                "recipient_devices": recipient_device_ids,
                "created_at": timestamp.isoformat(),
                "retry_count": 0,
                "last_attempt_at": None,
                "next_retry_in_seconds": self.RETRY_BACKOFF_SCHEDULE[0],  # Start at 2s
                "state": "pending"  # pending ‚Üí sent ‚Üí delivered ‚Üí read
            }
            
            # Store in device-local retry queue (Redis)
            retry_queue_key = f"{self.MESSAGE_RETRY_PREFIX}:{sender_user_id}:{sender_device_id}"
            # await self.redis.lpush(retry_queue_key, json.dumps(retry_entry))
            # await self.redis.expire(retry_queue_key, self.RETRY_QUEUE_TTL_SECONDS)
            
            # Also initialize message state tracking
            state_key = f"{self.MESSAGE_STATE_PREFIX}:{message_id}"
            state_data = {
                "message_id": message_id,
                "state": "pending",
                "created_at": timestamp.isoformat(),
                "delivered_at": None,
                "read_at": None,
                "failed_devices": []
            }
            # await self.redis.setex(state_key, self.MESSAGE_TTL_SECONDS, json.dumps(state_data))
            
            logger.info(f"üì§ Message queued for retry: {message_id} (retry_count=0)")
            
            return {
                "message_id": message_id,
                "status": "queued_for_retry",
                "retry_count": 0,
                "next_retry_at": (timestamp + timedelta(seconds=self.RETRY_BACKOFF_SCHEDULE[0])).isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to queue message for retry: {e}")
            raise E2EECryptoError(f"Retry queue failed: {e}")
    
    async def retry_pending_messages(
        self,
        sender_user_id: str,
        sender_device_id: str
    ) -> Dict:
        """
        Device comes online ‚Üí retry all pending messages.
        
        ONLINE FLOW:
        1. Device queries retry queue
        2. FOR EACH pending message:
           a. Check if retry should happen now (backoff elapsed?)
           b. If yes: attempt send
           c. If success: move to "sent"
           d. If fail: increment retry_count, update next_retry_time
           e. If max attempts exceeded: move to "failed"
        3. Return retry results
        
        Args:
            sender_user_id: User whose device coming online
            sender_device_id: Device coming online
            
        Returns:
            Retry results {succeeded: [...], failed: [...], rescheduled: [...]}
        """
        try:
            retry_queue_key = f"{self.MESSAGE_RETRY_PREFIX}:{sender_user_id}:{sender_device_id}"
            # retry_queue = await self.redis.lrange(retry_queue_key, 0, -1)
            
            results = {
                "succeeded": [],
                "failed": [],
                "rescheduled": [],
                "total_processed": 0
            }
            
            # For simulation, create empty list
            retry_queue = []
            
            for retry_entry_json in retry_queue:
                retry_entry = json.loads(retry_entry_json)
                message_id = retry_entry['message_id']
                retry_count = retry_entry['retry_count']
                
                now = datetime.now(timezone.utc)
                last_attempt = datetime.fromisoformat(retry_entry['last_attempt_at']) if retry_entry['last_attempt_at'] else now
                
                # Check if backoff period has elapsed
                backoff_seconds = self.RETRY_BACKOFF_SCHEDULE[min(retry_count, len(self.RETRY_BACKOFF_SCHEDULE) - 1)]
                next_retry_time = last_attempt + timedelta(seconds=backoff_seconds)
                
                if now < next_retry_time:
                    # Not yet time to retry
                    logger.debug(f"‚è≥ Message {message_id} retry scheduled for {next_retry_time.isoformat()}")
                    results['rescheduled'].append(message_id)
                    continue
                
                # Time to retry
                if retry_count >= self.MAX_RETRY_ATTEMPTS:
                    # Max attempts exceeded
                    logger.warning(f"‚ùå Message {message_id} exceeded max retries ({self.MAX_RETRY_ATTEMPTS})")
                    state_key = f"{self.MESSAGE_STATE_PREFIX}:{message_id}"
                    # await self.redis.hset(state_key, "state", "failed")
                    results['failed'].append(message_id)
                    # Remove from retry queue
                    # await self.redis.lrem(retry_queue_key, 0, retry_entry_json)
                    continue
                
                # Attempt send
                try:
                    send_result = await self.encrypt_and_send_message(
                        session_id=f"{sender_device_id}_{retry_entry['recipient_devices'][0]}_session",
                        plaintext=retry_entry['content'],
                        sender_user_id=sender_user_id,
                        sender_device_id=sender_device_id,
                        recipient_user_id=retry_entry['recipient_user_id'],
                        recipient_devices=retry_entry['recipient_devices']
                    )
                    
                    # Update message state to "sent"
                    state_key = f"{self.MESSAGE_STATE_PREFIX}:{message_id}"
                    # await self.redis.hset(state_key, "state", "sent")
                    logger.info(f"‚úÖ Message {message_id} retried successfully (attempt {retry_count + 1})")
                    results['succeeded'].append(message_id)
                    
                    # Remove from retry queue
                    # await self.redis.lrem(retry_queue_key, 0, retry_entry_json)
                
                except Exception as send_err:
                    # Retry failed, update backoff
                    logger.warning(f"‚ö†Ô∏è Message {message_id} retry attempt {retry_count + 1} failed: {send_err}")
                    retry_entry['retry_count'] += 1
                    retry_entry['last_attempt_at'] = now.isoformat()
                    
                    next_idx = min(retry_entry['retry_count'], len(self.RETRY_BACKOFF_SCHEDULE) - 1)
                    retry_entry['next_retry_in_seconds'] = self.RETRY_BACKOFF_SCHEDULE[next_idx]
                    
                    # Update retry queue entry
                    # await self.redis.lrem(retry_queue_key, 0, retry_entry_json)
                    # await self.redis.lpush(retry_queue_key, json.dumps(retry_entry))
                    results['rescheduled'].append(message_id)
                
                results['total_processed'] += 1
            
            logger.info(f"üìä Retry processing complete: {results['total_processed']} messages processed")
            return results
        
        except Exception as e:
            logger.error(f"Retry processing failed: {e}")
            raise E2EECryptoError(f"Retry processing error: {e}")
    
    async def track_delivery_receipt(
        self,
        message_id: str,
        recipient_user_id: str,
        recipient_device_id: str,
        receipt_type: str  # "delivered" or "read"
    ) -> Dict:
        """
        Track delivery receipt for message on specific device.
        
        RECEIPT FLOW:
        1. Sender sends message to recipient
        2. Recipient device receives ‚Üí sends "delivered" receipt
        3. Recipient opens message ‚Üí sends "read" receipt
        4. Sender receives receipt ‚Üí updates message state
        
        Args:
            message_id: Message ID
            recipient_user_id: Recipient user
            recipient_device_id: Recipient device that delivered/read
            receipt_type: "delivered" or "read"
            
        Returns:
            Receipt information
        """
        try:
            receipt_key = f"{self.DELIVERY_RECEIPT_PREFIX}:{message_id}"
            timestamp = datetime.now(timezone.utc)
            
            receipt_data = {
                "message_id": message_id,
                "recipient_user_id": recipient_user_id,
                "recipient_device_id": recipient_device_id,
                "receipt_type": receipt_type,
                "timestamp": timestamp.isoformat()
            }
            
            # Store receipt
            # await self.redis.lpush(receipt_key, json.dumps(receipt_data))
            # await self.redis.expire(receipt_key, self.MESSAGE_TTL_SECONDS)
            
            # Update message state
            state_key = f"{self.MESSAGE_STATE_PREFIX}:{message_id}"
            if receipt_type == "read":
                # await self.redis.hset(state_key, "read_at", timestamp.isoformat())
                # await self.redis.hset(state_key, "state", "read")
                logger.info(f"‚úì Message {message_id} marked as read by {recipient_device_id}")
            elif receipt_type == "delivered":
                # await self.redis.hset(state_key, "delivered_at", timestamp.isoformat())
                # await self.redis.hset(state_key, "state", "delivered")
                logger.info(f"‚úì Message {message_id} delivered to {recipient_device_id}")
            
            return {
                "message_id": message_id,
                "status": receipt_type,
                "receipt_timestamp": timestamp.isoformat(),
                "device_id": recipient_device_id
            }
        
        except Exception as e:
            logger.error(f"Failed to track delivery receipt: {e}")
            raise E2EECryptoError(f"Receipt tracking failed: {e}")
    
    async def get_message_state(self, message_id: str) -> Dict:
        """
        Get current state of message (pending/sent/delivered/read).
        
        Args:
            message_id: Message ID
            
        Returns:
            Current message state
        """
        try:
            state_key = f"{self.MESSAGE_STATE_PREFIX}:{message_id}"
            # state_data_json = await self.redis.get(state_key)
            
            # For simulation:
            state_data = {
                "message_id": message_id,
                "state": "sent",  # Would be loaded from Redis
                "created_at": datetime.now(timezone.utc).isoformat(),
                "delivered_at": None,
                "read_at": None,
                "failed_devices": []
            }
            
            return state_data
        
        except Exception as e:
            logger.error(f"Failed to get message state: {e}")
            raise E2EECryptoError(f"State query failed: {e}")
    
    async def mark_device_online(
        self,
        user_id: str,
        device_id: str
    ) -> Dict:
        """
        Mark device as online and trigger retry flush.
        
        DEVICE ONLINE EVENT:
        1. Device connects to server
        2. Sends "online" signal: user_id, device_id, timestamp
        3. Server marks device online in Redis
        4. Server triggers retry_pending_messages() for that device
        5. Device receives pending messages + queued retries
        
        Args:
            user_id: User ID
            device_id: Device ID coming online
            
        Returns:
            Online status + retry results
        """
        try:
            online_key = f"{self.DEVICE_ONLINE_PREFIX}:{user_id}:{device_id}"
            timestamp = datetime.now(timezone.utc)
            
            online_data = {
                "user_id": user_id,
                "device_id": device_id,
                "online_at": timestamp.isoformat(),
                "status": "online"
            }
            
            # Mark online in Redis (no expiry, or 5-min expiry to detect disconnects)
            # await self.redis.setex(online_key, 300, json.dumps(online_data))  # 5 min
            
            logger.info(f"‚úÖ Device online: {device_id} (user={user_id})")
            
            # Trigger retry flush
            retry_result = await self.retry_pending_messages(user_id, device_id)
            
            return {
                "device_id": device_id,
                "status": "online",
                "online_at": timestamp.isoformat(),
                "retry_result": retry_result
            }
        
        except Exception as e:
            logger.error(f"Failed to mark device online: {e}")
            raise E2EECryptoError(f"Online status update failed: {e}")
    
    async def mark_device_offline(
        self,
        user_id: str,
        device_id: str
    ) -> Dict:
        """
        Mark device as offline.
        
        Args:
            user_id: User ID
            device_id: Device ID going offline
            
        Returns:
            Offline status confirmation
        """
        try:
            online_key = f"{self.DEVICE_ONLINE_PREFIX}:{user_id}:{device_id}"
            timestamp = datetime.now(timezone.utc)
            
            # Remove from online tracking
            # await self.redis.delete(online_key)
            
            logger.info(f"‚è∏Ô∏è  Device offline: {device_id} (user={user_id})")
            
            return {
                "device_id": device_id,
                "status": "offline",
                "offline_at": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to mark device offline: {e}")
            raise E2EECryptoError(f"Offline status update failed: {e}")


# ======================== ABUSE & ANTI-SPAM SYSTEM ========================

class AbuseAndSpamScoringService:
    """
    Score-based abuse & anti-spam system (WhatsApp-style graduated enforcement).
    
    SCORING MODEL:
    - Score 0.0-1.0 scale (0 = clean, 1.0 = suspended)
    - Incremental violations accumulate
    - Decay: -0.1 per day of good behavior (rehabilitation path)
    - Progressive enforcement: shadow ban ‚Üí throttle ‚Üí suspension
    
    ACTIONS:
    - 0.0-0.5: Normal (learning phase)
    - 0.5-0.6: Shadow ban (messages queued, not delivered)
    - 0.6-0.9: Throttle (rate limits tightened)
    - 0.9-1.0: Suspended (account locked)
    """
    
    def __init__(self, db=None, redis_client=None):
        """Initialize abuse scoring service."""
        self.db = db
        self.redis = redis_client
        
        # Redis key prefixes
        self.ABUSE_SCORE_PREFIX = "abuse:score"
        self.VELOCITY_COUNTER_PREFIX = "abuse:velocity"
        self.UNIQUE_RECIPIENTS_PREFIX = "abuse:recipients"
        self.MODERATION_QUEUE_PREFIX = "abuse:moderation"
        
        # Scoring configuration (from kubernetes.yaml environment variables)
        self.MESSAGE_VELOCITY_LIMIT_PER_MINUTE = 100
        self.MESSAGE_VELOCITY_LIMIT_PER_HOUR = 1000
        self.UNIQUE_RECIPIENTS_HOURLY_LIMIT = 100
        self.UNIQUE_RECIPIENTS_DAILY_LIMIT = 1000
        
        # Score increments
        self.SPAM_KEYWORD_SCORE_INCREMENT = 0.1
        self.VELOCITY_VIOLATION_SCORE_INCREMENT = 0.15
        self.REPORT_SCORE_INCREMENT = 0.2
        self.PHISHING_LINK_SCORE_INCREMENT = 0.25
        
        # Thresholds for actions
        self.SHADOW_BAN_THRESHOLD = 0.6
        self.SUSPENSION_THRESHOLD = 0.9
        
        # Duration of actions (hours)
        self.SHADOW_BAN_DURATION_HOURS = 24
        self.SUSPENSION_DURATION_HOURS = 168  # 7 days
        
        # Score decay
        self.SCORE_DECAY_PER_DAY = 0.1
        
        # Moderation
        self.MODERATION_ENABLED = True
        self.AUTO_REPORT_THRESHOLD = 0.5  # Auto-escalate for manual review at 0.5
        self.EXPLICIT_CONTENT_DETECTION = True
        self.PHISHING_DETECTION = True
    
    async def get_user_abuse_score(self, user_id: str) -> Dict:
        """
        Get current abuse score for user.
        
        Args:
            user_id: User ID
            
        Returns:
            Current score + metadata
        """
        try:
            score_key = f"{self.ABUSE_SCORE_PREFIX}:{user_id}"
            # score_data = await self.redis.get(score_key)
            
            score_data = {
                "user_id": user_id,
                "score": 0.0,  # Would be loaded from Redis/MongoDB
                "created_at": datetime.now(timezone.utc).isoformat(),
                "last_updated_at": datetime.now(timezone.utc).isoformat(),
                "violations": [],
                "action": None,
                "action_expires_at": None
            }
            
            return score_data
        
        except Exception as e:
            logger.error(f"Failed to get abuse score: {e}")
            raise
    
    async def increment_abuse_score(
        self,
        user_id: str,
        violation_type: str,
        increment: float,
        reason: str = None
    ) -> Dict:
        """
        Increment abuse score for user.
        
        VIOLATION TYPES:
        - "spam_keyword": Explicit spam content detected (+0.1)
        - "velocity_violation": Too many messages in time window (+0.15)
        - "abuse_report": User reported by others (+0.2)
        - "phishing_link": Malicious link detected (+0.25)
        
        Args:
            user_id: User ID
            violation_type: Type of violation
            increment: Score to add (0.0-1.0)
            reason: Descriptive reason
            
        Returns:
            Updated score + new action
        """
        try:
            score_key = f"{self.ABUSE_SCORE_PREFIX}:{user_id}"
            timestamp = datetime.now(timezone.utc)
            
            # Get current score
            current_score_data = await self.get_user_abuse_score(user_id)
            old_score = current_score_data.get('score', 0.0)
            
            # Increment
            new_score = min(old_score + increment, 1.0)  # Cap at 1.0
            
            # Determine action based on threshold
            action = None
            action_expires_at = None
            
            if new_score >= self.SUSPENSION_THRESHOLD:
                action = "suspended"
                action_expires_at = (timestamp + timedelta(hours=self.SUSPENSION_DURATION_HOURS)).isoformat()
                logger.warning(f"üîí User {user_id} SUSPENDED: score={new_score:.2f}")
            
            elif new_score >= self.SHADOW_BAN_THRESHOLD:
                action = "shadow_banned"
                action_expires_at = (timestamp + timedelta(hours=self.SHADOW_BAN_DURATION_HOURS)).isoformat()
                logger.warning(f"üëÅÔ∏è User {user_id} SHADOW BANNED: score={new_score:.2f}")
            
            # Store update
            update_data = {
                "user_id": user_id,
                "old_score": old_score,
                "new_score": new_score,
                "violation_type": violation_type,
                "increment": increment,
                "reason": reason,
                "timestamp": timestamp.isoformat(),
                "action": action,
                "action_expires_at": action_expires_at
            }
            
            # await self.redis.setex(score_key, 2592000, json.dumps(update_data))  # 30 days TTL
            
            logger.info(f"‚ö†Ô∏è  Abuse score updated: {user_id} | old:{old_score:.2f} ‚Üí new:{new_score:.2f} | type:{violation_type}")
            
            # If auto-review enabled, add to moderation queue
            if self.MODERATION_ENABLED and new_score >= self.AUTO_REPORT_THRESHOLD:
                await self._add_to_moderation_queue(user_id, new_score, violation_type, reason)
            
            return update_data
        
        except Exception as e:
            logger.error(f"Failed to increment abuse score: {e}")
            raise
    
    async def check_message_velocity(
        self,
        user_id: str,
        message_count: int = 1
    ) -> Dict:
        """
        Check if user exceeds message velocity limits.
        
        VELOCITY LIMITS:
        - Per-minute: 100 messages/min
        - Per-hour: 1000 messages/hour
        - Unique recipients per-hour: 100
        - Unique recipients per-day: 1000
        
        Args:
            user_id: User ID
            message_count: Number of messages about to send
            
        Returns:
            Velocity check result {violated: bool, violations: [...]}
        """
        try:
            timestamp = datetime.now(timezone.utc)
            violations = []
            
            # 1. Check per-minute limit
            minute_key = f"{self.VELOCITY_COUNTER_PREFIX}:minute:{user_id}:{timestamp.strftime('%Y%m%d%H%M')}"
            # minute_count = await self.redis.incr(minute_key)
            # await self.redis.expire(minute_key, 120)  # Expire after 2 minutes
            minute_count = message_count  # Simulated
            
            if minute_count > self.MESSAGE_VELOCITY_LIMIT_PER_MINUTE:
                violations.append({
                    "type": "velocity_per_minute",
                    "limit": self.MESSAGE_VELOCITY_LIMIT_PER_MINUTE,
                    "current": minute_count
                })
                logger.warning(f"‚ö†Ô∏è  User {user_id} exceeded minute velocity: {minute_count} > {self.MESSAGE_VELOCITY_LIMIT_PER_MINUTE}")
            
            # 2. Check per-hour limit
            hour_key = f"{self.VELOCITY_COUNTER_PREFIX}:hour:{user_id}:{timestamp.strftime('%Y%m%d%H')}"
            # hour_count = await self.redis.incr(hour_key)
            # await self.redis.expire(hour_key, 3660)  # Expire after 61 minutes
            hour_count = message_count  # Simulated
            
            if hour_count > self.MESSAGE_VELOCITY_LIMIT_PER_HOUR:
                violations.append({
                    "type": "velocity_per_hour",
                    "limit": self.MESSAGE_VELOCITY_LIMIT_PER_HOUR,
                    "current": hour_count
                })
                logger.warning(f"‚ö†Ô∏è  User {user_id} exceeded hour velocity: {hour_count} > {self.MESSAGE_VELOCITY_LIMIT_PER_HOUR}")
            
            violated = len(violations) > 0
            
            return {
                "user_id": user_id,
                "violated": violated,
                "violations": violations,
                "checked_at": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to check message velocity: {e}")
            raise
    
    async def track_unique_recipients(
        self,
        user_id: str,
        recipient_user_ids: List[str]
    ) -> Dict:
        """
        Track unique recipients per-hour and per-day.
        
        LIMITS:
        - Per-hour: 100 unique recipients
        - Per-day: 1000 unique recipients
        
        Args:
            user_id: Sender user ID
            recipient_user_ids: List of recipient user IDs
            
        Returns:
            Tracking result {violations: [...]}
        """
        try:
            timestamp = datetime.now(timezone.utc)
            violations = []
            
            # 1. Track per-hour unique recipients
            hour_key = f"{self.UNIQUE_RECIPIENTS_PREFIX}:hour:{user_id}:{timestamp.strftime('%Y%m%d%H')}"
            # await self.redis.sadd(hour_key, *recipient_user_ids)
            # await self.redis.expire(hour_key, 3660)
            # hour_unique_count = await self.redis.scard(hour_key)
            hour_unique_count = len(recipient_user_ids)  # Simulated
            
            if hour_unique_count > self.UNIQUE_RECIPIENTS_HOURLY_LIMIT:
                violations.append({
                    "type": "recipients_per_hour",
                    "limit": self.UNIQUE_RECIPIENTS_HOURLY_LIMIT,
                    "current": hour_unique_count
                })
                logger.warning(f"‚ö†Ô∏è  User {user_id} exceeded hourly recipient limit: {hour_unique_count} > {self.UNIQUE_RECIPIENTS_HOURLY_LIMIT}")
            
            # 2. Track per-day unique recipients
            day_key = f"{self.UNIQUE_RECIPIENTS_PREFIX}:day:{user_id}:{timestamp.strftime('%Y%m%d')}"
            # await self.redis.sadd(day_key, *recipient_user_ids)
            # await self.redis.expire(day_key, 86400)
            # day_unique_count = await self.redis.scard(day_key)
            day_unique_count = len(recipient_user_ids)  # Simulated
            
            if day_unique_count > self.UNIQUE_RECIPIENTS_DAILY_LIMIT:
                violations.append({
                    "type": "recipients_per_day",
                    "limit": self.UNIQUE_RECIPIENTS_DAILY_LIMIT,
                    "current": day_unique_count
                })
                logger.warning(f"‚ö†Ô∏è  User {user_id} exceeded daily recipient limit: {day_unique_count} > {self.UNIQUE_RECIPIENTS_DAILY_LIMIT}")
            
            return {
                "user_id": user_id,
                "violations": violations,
                "hourly_unique": hour_unique_count,
                "daily_unique": day_unique_count,
                "checked_at": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to track unique recipients: {e}")
            raise
    
    async def process_abuse_report(
        self,
        reporter_user_id: str,
        reported_user_id: str,
        report_type: str,
        reason: str
    ) -> Dict:
        """
        Process abuse report from user (e.g., "Spam", "Harassment", "CSAM").
        
        REPORT TYPES:
        - "spam": Spam/unsolicited messages
        - "harassment": Abusive/threatening
        - "csam": Child safety concerns
        - "phishing": Malicious/phishing
        
        Args:
            reporter_user_id: User filing report
            reported_user_id: User being reported
            report_type: Type of abuse
            reason: Descriptive reason
            
        Returns:
            Report ID + action taken
        """
        try:
            report_id = f"report_{secrets.token_hex(16)}"
            timestamp = datetime.now(timezone.utc)
            
            # Create report object
            report_data = {
                "report_id": report_id,
                "reporter_user_id": reporter_user_id,
                "reported_user_id": reported_user_id,
                "report_type": report_type,
                "reason": reason,
                "created_at": timestamp.isoformat(),
                "status": "pending",  # pending ‚Üí investigating ‚Üí resolved
                "moderator_notes": None
            }
            
            # Store report
            moderation_key = f"{self.MODERATION_QUEUE_PREFIX}:{report_id}"
            # await self.redis.setex(moderation_key, 2592000, json.dumps(report_data))  # 30 days
            
            # Increment score for reported user
            await self.increment_abuse_score(
                user_id=reported_user_id,
                violation_type="abuse_report",
                increment=self.REPORT_SCORE_INCREMENT,
                reason=f"Report filed: {report_type}"
            )
            
            logger.info(f"üìã Abuse report created: {report_id} | reported:{reported_user_id} | type:{report_type}")
            
            return {
                "report_id": report_id,
                "status": "filed",
                "reported_user_id": reported_user_id,
                "created_at": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to process abuse report: {e}")
            raise
    
    async def _add_to_moderation_queue(
        self,
        user_id: str,
        score: float,
        violation_type: str,
        reason: str
    ) -> None:
        """
        Add user to manual moderation queue for human review.
        
        Args:
            user_id: User ID
            score: Current abuse score
            violation_type: Type of violation
            reason: Reason for review
        """
        try:
            review_data = {
                "user_id": user_id,
                "score": score,
                "violation_type": violation_type,
                "reason": reason,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "reviewed": False
            }
            
            queue_key = f"{self.MODERATION_QUEUE_PREFIX}:pending"
            # await self.redis.lpush(queue_key, json.dumps(review_data))
            
            logger.info(f"üì¨ User added to moderation queue: {user_id} (score={score:.2f})")
        
        except Exception as e:
            logger.error(f"Failed to add to moderation queue: {e}")
    
    async def apply_enforcement_action(
        self,
        user_id: str,
        action: str
    ) -> Dict:
        """
        Apply enforcement action based on score.
        
        ACTIONS:
        - "shadow_ban": Queue messages, don't deliver
        - "throttle": Rate limit to 10 msg/min
        - "suspend": Lock account, require support contact
        
        Args:
            user_id: User ID
            action: Action to apply
            
        Returns:
            Action confirmation
        """
        try:
            timestamp = datetime.now(timezone.utc)
            
            if action == "shadow_ban":
                # Messages go to queue but don't get delivered
                enforce_key = f"abuse:shadow_ban:{user_id}"
                # await self.redis.setex(enforce_key, 86400, "1")  # 24 hours
                logger.warning(f"üëÅÔ∏è Shadow ban applied: {user_id}")
            
            elif action == "throttle":
                # Tighten rate limits
                enforce_key = f"abuse:throttle:{user_id}"
                # await self.redis.setex(enforce_key, 3600, "1")  # 1 hour
                logger.warning(f"üö´ Throttle applied: {user_id}")
            
            elif action == "suspend":
                # Lock account
                enforce_key = f"abuse:suspended:{user_id}"
                # await self.redis.set(enforce_key, "1")  # Indefinite until removed
                logger.warning(f"üîí Suspension applied: {user_id}")
            
            return {
                "user_id": user_id,
                "action": action,
                "applied_at": timestamp.isoformat()
            }
        
        except Exception as e:
            logger.error(f"Failed to apply enforcement action: {e}")
            raise
    
    async def decay_abuse_score(self, user_id: str) -> Dict:
        """
        Decay abuse score by 0.1 per day of good behavior (rehabilitation).
        
        SCHEDULE:
        - Scores decay automatically each day
        - Run via Kubernetes CronJob daily
        - User with 0.5 score: after 5 days ‚Üí 0.0 (fully rehabilitated)
        
        Args:
            user_id: User ID
            
        Returns:
            Updated score after decay
        """
        try:
            current_score = (await self.get_user_abuse_score(user_id)).get('score', 0.0)
            new_score = max(current_score - self.SCORE_DECAY_PER_DAY, 0.0)
            
            # Update score
            score_key = f"{self.ABUSE_SCORE_PREFIX}:{user_id}"
            # await self.redis.setex(score_key, 2592000, json.dumps({"score": new_score}))
            
            logger.info(f"üìâ Abuse score decayed: {user_id} | old:{current_score:.2f} ‚Üí new:{new_score:.2f}")
            
            # If score drops below shadow ban threshold, lift shadow ban
            if current_score >= self.SHADOW_BAN_THRESHOLD and new_score < self.SHADOW_BAN_THRESHOLD:
                await self.apply_enforcement_action(user_id, "lift_shadow_ban")
            
            return {
                "user_id": user_id,
                "old_score": current_score,
                "new_score": new_score,
                "decayed_by": self.SCORE_DECAY_PER_DAY
            }
        
        except Exception as e:
            logger.error(f"Failed to decay abuse score: {e}")
            raise


async def get_e2ee_service(db=None, redis_client=None) -> E2EEService:
    """Factory function to get E2EE service instance."""
    return E2EEService(db=db, redis_client=redis_client)


async def get_abuse_service(db=None, redis_client=None) -> AbuseAndSpamScoringService:
    """Factory function to get abuse scoring service instance."""
    return AbuseAndSpamScoringService(db=db, redis_client=redis_client)


# ======================== GROUP CHAT ENCRYPTION (SIGNAL SENDER KEYS) ========================

class GroupEncryptionService:
    """
    Group chat encryption using Signal Protocol Sender Keys.
    
    WHATSAPP GROUP ENCRYPTION:
    - One sender key per group member (not shared)
    - Reduces key material: O(1) per sender instead of O(recipients)
    - Group ratchet: derives per-device keys for each recipient device
    - Scales to large groups without key explosion
    
    FLOW:
    1. Group created ‚Üí admin generates sender key
    2. Admin distributes sender key to all members (via 1-to-1 E2EE)
    3. Member sends group message:
       a. Encrypt with their own sender key
       b. Server performs fan-out to recipient devices
       c. Each device receives encrypted message
       d. Decrypts with admin's published sender key
    """
    
    def __init__(self, db=None, redis_client=None):
        """Initialize group encryption service."""
        self.db = db
        self.redis = redis_client
        
        # Redis key prefixes
        self.SENDER_KEY_PREFIX = "group:sender_key"
        self.GROUP_STATE_PREFIX = "group:state"
        self.SEQUENCE_PREFIX = "group:sequence"
    
    async def create_group_sender_key(
        self,
        group_id: str,
        sender_user_id: str,
        sender_device_id: str
    ) -> Dict:
        """
        Create sender key for group member (initiator side).
        
        FLOW:
        1. Generate random seed (256-bit)
        2. Derive sender key material via KDF
        3. Store in Redis (per-user, per-device)
        4. Return public key for distribution
        
        Args:
            group_id: Group chat ID
            sender_user_id: User sending in group
            sender_device_id: User's device
            
        Returns:
            Sender key info (public only, private stored locally)
        """
        try:
            seed = secrets.token_bytes(32)
            seed_b64 = base64.b64encode(seed).decode()
            sender_key_id = 0
            
            logger.info(f"üîê Group sender key created: {group_id} | sender:{sender_device_id}")
            
            return {
                "group_id": group_id,
                "sender_user_id": sender_user_id,
                "sender_device_id": sender_device_id,
                "sender_key_id": sender_key_id,
                "seed_b64": seed_b64,
                "message": "‚úì Sender key created"
            }
        
        except Exception as e:
            logger.error(f"Failed to create group sender key: {e}")
            raise
    
    async def get_group_message_sequence(
        self,
        group_id: str
    ) -> int:
        """
        Get next message sequence number for group (strict ordering).
        
        WHATSAPP GROUP ORDERING:
        - Each group has monotonic sequence
        - Prevents message reordering
        - Detects missing messages
        
        Args:
            group_id: Group chat ID
            
        Returns:
            Next sequence number for message
        """
        try:
            seq_key = f"{self.SEQUENCE_PREFIX}:{group_id}"
            # next_seq = await self.redis.incr(seq_key)
            # await self.redis.expire(seq_key, 2592000)  # 30 days
            
            # For simulation:
            next_seq = 1
            logger.debug(f"üìä Group sequence: {group_id} ‚Üí {next_seq}")
            
            return next_seq
        
        except Exception as e:
            logger.error(f"Failed to get group sequence: {e}")
            raise
    
    async def fanout_group_message(
        self,
        group_id: str,
        sender_user_id: str,
        sender_device_id: str,
        message_id: str,
        ciphertext_b64: str,
        recipient_user_ids: List[str],
        sequence_number: int
    ) -> Dict:
        """
        Fan-out group message to all member devices.
        
        WHATSAPP GROUP FAN-OUT:
        1. Message encrypted once (with sender key)
        2. Server performs per-device re-encryption FOR EACH member
        3. Each member's device gets unique ciphertext
        4. Sequence number enforced
        
        Args:
            group_id: Group ID
            sender_user_id: Who sent
            sender_device_id: Which device
            message_id: Message ID
            ciphertext_b64: Encrypted message
            recipient_user_ids: All group members (excluding sender)
            sequence_number: Monotonic sequence in group
            
        Returns:
            Fan-out status per device
        """
        try:
            fanout_status = {
                "message_id": message_id,
                "group_id": group_id,
                "sequence": sequence_number,
                "recipient_devices": {},
                "failed_devices": []
            }
            
            for recipient_user_id in recipient_user_ids:
                if recipient_user_id == sender_user_id:
                    continue  # Don't send to sender
                
                # In real impl: fetch recipient's device list, encrypt separately
                fanout_status["recipient_devices"][recipient_user_id] = {
                    "status": "queued",
                    "sequence": sequence_number,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            
            logger.info(f"üì¢ Group message fanned out: {message_id} ‚Üí {len(fanout_status['recipient_devices'])} members")
            
            return fanout_status
        
        except Exception as e:
            logger.error(f"Failed to fanout group message: {e}")
            raise


# ======================== PRESENCE & TYPING SERVICE ========================

class PresenceAndTypingService:
    """
    Presence (online/offline) and typing indicator service.
    
    WHATSAPP PRESENCE:
    - Minimal metadata (just online/offline/away)
    - Privacy: show last seen only to contacts
    - Typing indicator via ephemeral Redis pub/sub
    - Broadcast via WebSocket
    """
    
    def __init__(self, db=None, redis_client=None):
        """Initialize presence service."""
        self.db = db
        self.redis = redis_client
        
        # Redis prefixes
        self.PRESENCE_PREFIX = "presence"
        self.TYPING_PREFIX = "typing"
        self.PRESENCE_PUB_SUB = "presence_updates"
        self.TYPING_PUB_SUB = "typing_updates"
    
    async def broadcast_presence(
        self,
        user_id: str,
        device_id: str,
        status: str,  # online, offline, away
        show_last_seen: bool = True
    ) -> Dict:
        """
        Broadcast presence to contacts.
        
        Args:
            user_id: User broadcasting
            device_id: Device
            status: online/offline/away
            show_last_seen: Privacy control
            
        Returns:
            Presence update info
        """
        try:
            timestamp = datetime.now(timezone.utc)
            presence_key = f"{self.PRESENCE_PREFIX}:{user_id}:{device_id}"
            
            presence_data = {
                "user_id": user_id,
                "device_id": device_id,
                "status": status,
                "timestamp": timestamp.isoformat(),
                "show_last_seen": show_last_seen
            }
            
            # await self.redis.setex(presence_key, 300, json.dumps(presence_data))  # 5 min TTL
            # await self.redis.publish(self.PRESENCE_PUB_SUB, json.dumps(presence_data))
            
            logger.info(f"üìç Presence: {user_id}@{device_id} ‚Üí {status}")
            
            return presence_data
        
        except Exception as e:
            logger.error(f"Failed to broadcast presence: {e}")
            raise
    
    async def broadcast_typing(
        self,
        chat_id: str,
        user_id: str,
        device_id: str,
        is_typing: bool = True
    ) -> Dict:
        """
        Broadcast typing indicator to chat members.
        
        WHATSAPP TYPING:
        - Ephemeral (3-min TTL)
        - Pub/sub broadcast (no DB storage)
        - Auto-clears if user stops typing
        
        Args:
            chat_id: Chat where typing
            user_id: User typing
            device_id: Device
            is_typing: True if typing, False if stopped
            
        Returns:
            Typing indicator update
        """
        try:
            timestamp = datetime.now(timezone.utc)
            typing_key = f"{self.TYPING_PREFIX}:{chat_id}:{user_id}:{device_id}"
            
            typing_data = {
                "chat_id": chat_id,
                "user_id": user_id,
                "device_id": device_id,
                "is_typing": is_typing,
                "timestamp": timestamp.isoformat()
            }
            
            if is_typing:
                # await self.redis.setex(typing_key, 180, json.dumps(typing_data))  # 3 min TTL
                # await self.redis.publish(f"{self.TYPING_PUB_SUB}:{chat_id}", json.dumps(typing_data))
                logger.debug(f"‚å®Ô∏è  User typing: {user_id}@{chat_id}")
            else:
                # await self.redis.delete(typing_key)
                logger.debug(f"‚úì User stopped typing: {user_id}@{chat_id}")
            
            return typing_data
        
        except Exception as e:
            logger.error(f"Failed to broadcast typing: {e}")
            raise


# ======================== BACKGROUND WORKERS ========================

class BackgroundWorkerCoordinator:
    """
    Coordinates background jobs for reliable message delivery.
    
    WORKERS:
    1. Message Fanout: Send to all recipient devices
    2. Retry Worker: Process pending retries with backoff
    3. Typing Cleanup: Remove expired typing indicators
    4. Group Key Distribution: Send sender keys to new members
    """
    
    def __init__(self, db=None, redis_client=None):
        """Initialize background coordinator."""
        self.db = db
        self.redis = redis_client
        
        # Redis queues
        self.FANOUT_QUEUE = "queue:fanout"
        self.RETRY_QUEUE = "queue:retry"
        self.GROUP_KEY_DIST_QUEUE = "queue:group_key_dist"
    
    async def enqueue_fanout_job(
        self,
        message_id: str,
        sender_user_id: str,
        recipient_user_ids: List[str],
        ciphertext_b64: str
    ) -> str:
        """
        Enqueue message fanout job.
        
        Args:
            message_id: Message to fanout
            sender_user_id: Sender
            recipient_user_ids: Recipients
            ciphertext_b64: Encrypted content
            
        Returns:
            Job ID
        """
        try:
            job_id = f"fanout_{message_id}"
            job_data = {
                "job_id": job_id,
                "message_id": message_id,
                "sender_user_id": sender_user_id,
                "recipient_user_ids": recipient_user_ids,
                "ciphertext_b64": ciphertext_b64,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "status": "pending"
            }
            
            # await self.redis.lpush(self.FANOUT_QUEUE, json.dumps(job_data))
            logger.debug(f"üì® Fanout job queued: {job_id}")
            
            return job_id
        
        except Exception as e:
            logger.error(f"Failed to enqueue fanout: {e}")
            raise
    
    async def enqueue_retry_job(
        self,
        message_id: str,
        sender_user_id: str,
        retry_count: int = 0
    ) -> str:
        """
        Enqueue message retry job.
        
        Args:
            message_id: Message to retry
            sender_user_id: Sender
            retry_count: Current retry attempt
            
        Returns:
            Job ID
        """
        try:
            job_id = f"retry_{message_id}_{retry_count}"
            job_data = {
                "job_id": job_id,
                "message_id": message_id,
                "sender_user_id": sender_user_id,
                "retry_count": retry_count,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "status": "pending"
            }
            
            # await self.redis.lpush(self.RETRY_QUEUE, json.dumps(job_data))
            logger.debug(f"üîÑ Retry job queued: {job_id}")
            
            return job_id
        
        except Exception as e:
            logger.error(f"Failed to enqueue retry: {e}")
            raise
    
    async def cleanup_expired_typing(self) -> Dict:
        """
        Clean up expired typing indicators (background job).
        
        SCHEDULE:
        - Runs every 1 minute
        - Removes typing entries with expired TTL
        
        Returns:
            Cleanup statistics
        """
        try:
            # In production: scan all typing keys, delete expired
            # For now: track that this ran
            logger.debug(f"üßπ Typing cleanup executed")
            
            return {
                "status": "completed",
                "cleaned_up": 0,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        except Exception as e:
            logger.error(f"Typing cleanup failed: {e}")
            raise


# ==================== BACKGROUND JOB EXECUTORS (NEW) ====================

class FanoutJobExecutor:
    """Execute per-device message fanout jobs"""
    
    def __init__(self, db=None, redis_client=None):
        self.db = db
        self.redis = redis_client
    
    async def execute_fanout_job(self, job_id: str) -> Dict:
        """
        Fan-out encrypted message to all recipient devices.
        
        ALGORITHM:
        1. Fetch message from queue:fanout
        2. For each recipient user:
           a. List all devices for recipient
           b. Generate unique DH key per device
           c. Encrypt message body with per-device key
           d. Store in device-specific Redis queue
           e. Mark as delivered
        3. Delete job from queue
        
        WHATSAPP FANOUT:
        - Each device gets unique ciphertext (DH-derived key)
        - O(recipients √ó devices) operations (acceptable for fanout)
        - Idempotent (can retry safely)
        """
        try:
            logger.info(f"üöÄ Executing fanout job: {job_id}")
            
            job_data = await self.redis.get(f"job:{job_id}")
            if not job_data:
                logger.warning(f"Job not found: {job_id}")
                return {"status": "not_found"}
            
            job_dict = json.loads(job_data) if isinstance(job_data, str) else job_data
            params = job_dict.get("parameters", {})
            message_id = params.get("message_id")
            recipient_user_ids = params.get("recipients", [])
            
            # For each recipient: fetch devices, encrypt per-device, queue delivery
            for recipient_user_id in recipient_user_ids:
                # In production: fetch devices from DB
                device_ids = await self._get_user_devices(recipient_user_id)
                
                for device_id in device_ids:
                    # Simulate per-device encryption (in production: use X3DH session)
                    per_device_key = secrets.token_hex(32)
                    encrypted_payload = f"encrypted_for_{device_id}_{per_device_key}"
                    
                    # Queue for device
                    queue_key = f"device:{recipient_user_id}:{device_id}:messages"
                    await self.redis.lpush(queue_key, encrypted_payload)
                    
                    # Update delivery state
                    await self.redis.set(
                        f"delivery:{message_id}:{recipient_user_id}:{device_id}",
                        "delivered",
                        ex=3600
                    )
            
            # Mark job as completed
            logger.info(f"‚úì Fanout complete: {message_id}")
            return {"status": "completed", "message_id": message_id}
        
        except Exception as e:
            logger.error(f"Fanout job failed: {e}")
            return {"status": "error", "error": str(e)}
    
    async def _get_user_devices(self, user_id: str) -> List[str]:
        """Fetch list of active devices for user"""
        # In production: query from device_sessions in DB/Redis
        devices = await self.redis.smembers(f"user:{user_id}:devices")
        return list(devices) if devices else []


class RetryJobExecutor:
    """Execute message retry jobs with exponential backoff"""
    
    def __init__(self, db=None, redis_client=None):
        self.db = db
        self.redis = redis_client
    
    async def execute_retry_job(self, job_id: str) -> Dict:
        """
        Retry failed message delivery with exponential backoff.
        
        ALGORITHM:
        1. Fetch job from queue:retry
        2. Check if retry_count < MAX_RETRIES (5)
        3. Attempt delivery to device
        4. If fails:
           a. Increment attempt counter
           b. Requeue with longer backoff
           c. Mark as pending
        5. If succeeds:
           a. Update delivery state to delivered
           b. Delete from retry queue
        
        WHATSAPP RETRY:
        - Exponential backoff: 2^attempt seconds (2, 4, 8, 16, 32...)
        - Max 5 retries (32s wait after final)
        - Preserves message ordering
        - Idempotent via job_id
        """
        try:
            logger.info(f"üîÑ Executing retry job: {job_id}")
            
            job_data = await self.redis.get(f"job:{job_id}")
            if not job_data:
                logger.warning(f"Job not found: {job_id}")
                return {"status": "not_found"}
            
            job_dict = json.loads(job_data) if isinstance(job_data, str) else job_data
            params = job_dict.get("parameters", {})
            message_id = params.get("message_id")
            device_id = params.get("device")
            attempt = params.get("attempt", 1)
            
            if attempt > 5:
                # Max retries exceeded
                logger.warning(f"Max retries exceeded for {message_id}:{device_id}")
                return {"status": "max_retries_exceeded"}
            
            # Attempt delivery
            delivery_ok = await self._attempt_device_delivery(message_id, device_id)
            
            if delivery_ok:
                logger.info(f"‚úì Retry successful: {message_id}:{device_id}")
                return {"status": "delivered"}
            else:
                # Requeue with incremented attempt
                next_attempt = attempt + 1
                backoff_seconds = min(2 ** next_attempt, 60)
                
                new_job = {
                    "job_id": f"retry_{secrets.token_hex(8)}",
                    "job_type": "retry",
                    "parameters": {"message_id": message_id, "device": device_id, "attempt": next_attempt},
                    "status": "pending",
                    "next_retry_at": (datetime.now(timezone.utc) + timedelta(seconds=backoff_seconds)).isoformat(),
                    "attempt_count": next_attempt
                }
                await self.redis.set(f"job:{new_job['job_id']}", json.dumps(new_job), ex=86400)
                await self.redis.lpush("queue:retry", new_job["job_id"])
                
                logger.info(f"Retry requeued: {message_id}:{device_id} (attempt {next_attempt})")
                return {"status": "requeued", "next_attempt": next_attempt}
        
        except Exception as e:
            logger.error(f"Retry job failed: {e}")
            return {"status": "error", "error": str(e)}
    
    async def _attempt_device_delivery(self, message_id: str, device_id: str) -> bool:
        """Attempt to deliver message to device"""
        # In production: check device online status, attempt push/WebSocket delivery
        try:
            device_online = await self.redis.get(f"device:{device_id}:online")
            return device_online == b"true"
        except:
            return False


class TypingCleanupExecutor:
    """Clean up expired typing indicators periodically"""
    
    def __init__(self, db=None, redis_client=None):
        self.db = db
        self.redis = redis_client
    
    async def execute_typing_cleanup(self) -> Dict:
        """
        Scan and remove expired typing indicators.
        
        ALGORITHM:
        1. Scan all keys matching typing:*
        2. Check Redis TTL for each
        3. Delete expired keys
        
        FREQUENCY: Run every 30 seconds
        IMPACT: Prevents typing indicator accumulation
        """
        try:
            logger.info("üßπ Typing cleanup started")
            
            cursor = 0
            deleted_count = 0
            
            while True:
                cursor, keys = await self.redis.scan(cursor, match="typing:*", count=100)
                
                for key in keys:
                    ttl = await self.redis.ttl(key)
                    if ttl < 0:
                        await self.redis.delete(key)
                        deleted_count += 1
                
                if cursor == 0:
                    break
            
            logger.info(f"‚úì Typing cleanup complete: {deleted_count} keys removed")
            return {"status": "completed", "deleted": deleted_count}
        
        except Exception as e:
            logger.error(f"Typing cleanup failed: {e}")
            return {"status": "error", "error": str(e)}


class GroupKeyDistributionExecutor:
    """Distribute sender keys to new group members"""
    
    def __init__(self, db=None, redis_client=None):
        self.db = db
        self.redis = redis_client
    
    async def execute_key_distribution(self, job_id: str) -> Dict:
        """
        Distribute sender keys to newly added group members.
        
        ALGORITHM:
        1. Fetch job from queue:group_key_dist
        2. Get group sender key (from group:sender_key)
        3. For each new member:
           a. Fetch member's public key
           b. Encrypt sender key with member's public key (via 1-to-1 session)
           c. Queue delivery to member
        4. Mark distribution complete
        5. Delete job
        
        WHATSAPP GROUP KEY DISTRIBUTION:
        - Sent via 1-to-1 E2EE to each member
        - Ensures new members can decrypt old group messages
        - Signed by group admin
        """
        try:
            logger.info(f"üì¶ Group key distribution job: {job_id}")
            
            job_data = await self.redis.get(f"job:{job_id}")
            if not job_data:
                logger.warning(f"Job not found: {job_id}")
                return {"status": "not_found"}
            
            job_dict = json.loads(job_data) if isinstance(job_data, str) else job_data
            params = job_dict.get("parameters", {})
            group_id = params.get("group_id")
            new_member_ids = params.get("new_members", [])
            
            # Fetch group sender key
            sender_key_data = await self.redis.get(f"group:sender_key:{group_id}")
            if not sender_key_data:
                logger.error(f"Sender key not found for group: {group_id}")
                return {"status": "key_not_found"}
            
            # For each new member: wrap key and queue delivery
            for member_id in new_member_ids:
                # In production: use member's public key to wrap sender key
                wrapped_key = f"wrapped_{secrets.token_hex(16)}"
                
                # Queue 1-to-1 delivery
                delivery_queue = f"user:{member_id}:key_delivery"
                await self.redis.lpush(delivery_queue, wrapped_key)
                
                logger.info(f"‚úì Sender key queued for {member_id}")
            
            logger.info(f"‚úì Group key distribution complete: {group_id}")
            return {"status": "completed", "group_id": group_id}
        
        except Exception as e:
            logger.error(f"Group key distribution failed: {e}")
            return {"status": "error", "error": str(e)}


async def get_fanout_executor(db=None, redis_client=None) -> FanoutJobExecutor:
    """Factory for fanout job executor."""
    return FanoutJobExecutor(db=db, redis_client=redis_client)


async def get_retry_executor(db=None, redis_client=None) -> RetryJobExecutor:
    """Factory for retry job executor."""
    return RetryJobExecutor(db=db, redis_client=redis_client)


async def get_typing_cleanup_executor(db=None, redis_client=None) -> TypingCleanupExecutor:
    """Factory for typing cleanup executor."""
    return TypingCleanupExecutor(db=db, redis_client=redis_client)


async def get_group_key_distribution_executor(db=None, redis_client=None) -> GroupKeyDistributionExecutor:
    """Factory for group key distribution executor."""
    return GroupKeyDistributionExecutor(db=db, redis_client=redis_client)



    """Factory for group encryption service."""
    return GroupEncryptionService(db=db, redis_client=redis_client)


async def get_presence_and_typing_service(db=None, redis_client=None) -> PresenceAndTypingService:
    """Factory for presence service."""
    return PresenceAndTypingService(db=db, redis_client=redis_client)


async def get_background_worker_coordinator(db=None, redis_client=None) -> BackgroundWorkerCoordinator:
    """Factory for background worker coordinator."""
    return BackgroundWorkerCoordinator(db=db, redis_client=redis_client)
